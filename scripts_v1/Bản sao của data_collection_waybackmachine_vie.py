
import os
import sys
import requests as rq
import time  


import json
from tqdm import tqdm
import os
import argparse
import re
import sys
sys.path.insert(1, os.path.join(sys.path[0], '..'))
from utils import *
from dataset_collection.scrape_utils import *
import requests

def collect_data_wayback(domain_url,
                         output_file,
                         start_date,
                         end_date,
                         max_urls=1000,
                         chunk_size=100,
                         sleep=10):
    os.makedirs(os.path.dirname(output_file), exist_ok=True)

    collected_urls = set()
    offset = 0
    headers = {'User-Agent': 'Mozilla/5.0'}
    no_progress_streak = 0  # Äáº¿m sá»‘ láº§n liÃªn tiáº¿p khÃ´ng thu tháº­p Ä‘Æ°á»£c thÃªm URL má»›i

    print(f"âœ¨ Äang thu tháº­p tá»«: {domain_url} (tá»« {start_date} Ä‘áº¿n {end_date})")

    while len(collected_urls) < max_urls:
        url = (
            f"https://web.archive.org/cdx/search/cdx"
            f"?url={domain_url}*"
            f"&from={start_date}&to={end_date}"
            f"&output=json&fl=original&collapse=urlkey"
            f"&filter=statuscode:200&limit={chunk_size}&offset={offset}"
        )

        print(f"ğŸ” Äang truy váº¥n offset {offset}...")
        try:
            res = requests.get(url, headers=headers, timeout=60)
            if res.status_code != 200 or not res.text.strip().startswith('['):
                print(f"âš ï¸ Pháº£n há»“i khÃ´ng há»£p lá»‡ (status: {res.status_code})")
                break
        except Exception as e:
            print(f"âŒ Lá»—i truy váº¥n offset {offset}: {e}")
            break

        try:
            data = res.json()
            urls_fetched = 0

            if len(data) <= 1:
                print("â›”ï¸ KhÃ´ng cÃ²n dá»¯ liá»‡u Ä‘á»ƒ thu tháº­p.")
                break

            for row in data[1:]:
                if isinstance(row, list) and len(row) > 0:
                    url_candidate = row[0]
                    if not any(x in url_candidate for x in ['/feed/', '/amp', '?__twitter_']):
                        if url_candidate not in collected_urls:
                            collected_urls.add(url_candidate)
                            urls_fetched += 1

            print(f"âœ”ï¸ Offset {offset} nháº­n Ä‘Æ°á»£c {urls_fetched} URL má»›i")

            # Náº¿u sá»‘ lÆ°á»£ng URL má»›i thu Ä‘Æ°á»£c < chunk_size => gáº§n háº¿t rá»“i, dá»«ng láº¡i
            if urls_fetched < chunk_size:
                no_progress_streak += 1
            else:
                no_progress_streak = 0

            if no_progress_streak >= 3:
                print("ğŸ“‰ KhÃ´ng cÃ³ thÃªm URL má»›i trong 3 lÆ°á»£t liÃªn tiáº¿p, káº¿t thÃºc.")
                break

            offset += chunk_size
            time.sleep(sleep)

        except Exception as e:
            print(f"âŒ Lá»—i khi xá»­ lÃ½ dá»¯ liá»‡u offset {offset}: {e}")
            break

    print(f"\nâœ… ÄÃ£ thu tháº­p Ä‘Æ°á»£c {len(collected_urls)} URL duy nháº¥t.")
    with open(output_file, 'w', encoding='utf-8') as f:
        for u in sorted(collected_urls):
            f.write(u + '\n')

def extract_basename(url):
    """
    Táº¡o tÃªn ngáº¯n gá»n tá»« URL Ä‘á»ƒ dÃ¹ng lÃ m tÃªn file lÆ°u bÃ i viáº¿t.
    """
    basename = url.strip().rstrip('/').split('/')[-1]
    # Náº¿u basename trá»‘ng, láº¥y tÃªn theo hash
    if not basename:
        import hashlib
        return hashlib.md5(url.encode('utf-8')).hexdigest()
    return basename

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Download articles and images from the Wayback machine.')
    parser.add_argument('--url_domain', type=str, default='vietfactcheck.org/', help='The domain to query.')
    parser.add_argument('--org', type=str, default='vietfactcheck', help='FC organization')
    parser.add_argument('--file_path', type=str, default='dataset/url/vietfactcheck.txt')
    parser.add_argument('--only_vietnamese', type=int, default=1, help='Chá»‰ thu tháº­p bÃ i viáº¿t tiáº¿ng Viá»‡t')
    parser.add_argument('--scrape_image', type=int, default=1)
    parser.add_argument('--process_image', type=int, default=1)
    parser.add_argument('--image_processing_script', type=str, default='dataset/image_processing_instructions.txt')
    parser.add_argument('--start_date', type=int, default=20200801)
    parser.add_argument('--end_date', type=int, default=20250801)
    parser.add_argument('--max_count', type=int, default=3000)
    parser.add_argument('--chunk_size', type=int, default=600)
    parser.add_argument('--sleep', type=int, default=10)

    args = parser.parse_args()

    collect_data_wayback(args.url_domain,
                         args.file_path,
                         start_date=args.start_date,
                         end_date=args.end_date,
                         max_urls=args.max_count,
                         chunk_size=args.chunk_size,
                         sleep=args.sleep)

    urls_all = load_urls(args.file_path)
    print(f"\U0001F4DD Tá»•ng URL thu tháº­p Ä‘Æ°á»£c: {len(urls_all)}")

    filtered_urls = [u for u in urls_all if is_valid_article_url(u)]
    print(f"\U0001F9F9 ÄÃ£ lá»c Ä‘Æ°á»£c {len(filtered_urls)} bÃ i viáº¿t há»£p lá»‡")
    # CÃ¡c bÃ i viáº¿t Ä‘Ã£ cÃ³ ná»™i dung (trÃ¡nh scrape láº¡i)
    existing_articles = {
        os.path.splitext(f)[0]
        for f in os.listdir('dataset/article/')
        if f.endswith('.txt')
    }

    # Giá»¯ láº¡i bÃ i viáº¿t chÆ°a scrape
    filtered_urls = [
        u for u in filtered_urls
        if extract_basename(u) not in existing_articles
    ]

    print(f"ğŸ“‚ ÄÃ£ bá» qua {len(urls_all) - len(filtered_urls)} bÃ i viáº¿t Ä‘Ã£ thu tháº­p.")

    urls_with_images = []
    failed_urls = []
    fail_reasons = {}
    success_count = 0

    for url in tqdm(filtered_urls):
        try:
            content, image_urls, basename = vietfactcheck_parser(url, only_vietnamese=bool(args.only_vietnamese))
            if not image_urls:
                failed_urls.append(url)
                fail_reasons[url] = 'no_image_found'
                continue
            urls_with_images.append((url, content, image_urls[:1], basename))
        except Exception as e:
            failed_urls.append(url)
            fail_reasons[url] = str(e)

    print(f"\nâœ… Tá»•ng bÃ i viáº¿t cÃ³ áº£nh: {len(urls_with_images)}")

    os.makedirs('dataset/article/', exist_ok=True)

    for i, (url, content, image_urls, basename) in enumerate(urls_with_images):
        file_name = f"vietfact_{basename}.txt"
        success = False

        if args.scrape_image:
            for j, img_url in enumerate(image_urls):
                success = scrape_image(img_url, basename)

        if success:
            with open(f"dataset/article/{file_name}", 'w', encoding='utf-8') as f:
                f.write(content)
            success_count += 1
        else:
            failed_urls.append(url)
            fail_reasons[url] = 'image_download_failed'

    # In thá»‘ng kÃª cho bÃ¡o cÃ¡o
    print("\nğŸ“Š Thá»‘ng kÃª quÃ¡ trÃ¬nh thu tháº­p:")
    print(f"- Tá»•ng URL ban Ä‘áº§u: {len(urls_all)}")
    print(f"- BÃ i viáº¿t há»£p lá»‡ sau lá»c: {len(filtered_urls) + len(existing_articles)}")
    print(f"- BÃ i viáº¿t Ä‘Ã£ tá»“n táº¡i (bá» qua): {len(existing_articles)}")
    print(f"- BÃ i viáº¿t má»›i cáº§n xá»­ lÃ½: {len(filtered_urls)}")
    print(f"- ThÃ nh cÃ´ng (cÃ³ ná»™i dung + áº£nh): {success_count}")
    print(f"- Tháº¥t báº¡i: {len(failed_urls)}")

    from collections import Counter
    reason_stats = Counter(fail_reasons.values())
    print("\nğŸ§¾ Thá»‘ng kÃª lá»—i:")
    for reason, count in reason_stats.items():
        print(f"- {reason}: {count} trÆ°á»ng há»£p")
